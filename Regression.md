## Regression（回归模型）
机器学习算法一般步骤：

1. 用数学语言描述一个问题，然后建立一个模型，例如回归模型或分类模型来描述这个问题
2. 通过最大似然，最大后验概率或者最小化分类误差等等建立模型的代价函数，也就是一个最优化问题。找到最优化问题的解，也就是能拟合我们的数据的最好的模型参数
3. 求解代价函数，找到最优解
	 - 如果这个代价函数存在解析解，对该函数求导，找到导数为0的点。
	 - 如果式子很难求导，例如函数里面存在隐含的变量或者变量相互间存在耦合，也就互相依赖的情况。或者求导后式子得不到解释解，例如未知参数的个数大于已知方程组的个数等。这时候我们就需要借助迭代算法来一步一步找到最有解了。迭代是个很神奇的东西，它将远大的目标（也就是找到最优的解，例如爬上山顶）记在心上，然后给自己定个短期目标（也就是每走一步，就离远大的目标更近一点），脚踏实地，心无旁贷，像个蜗牛一样，一步一步往上爬，支撑它的唯一信念是：只要我每一步都爬高一点，那么积跬步，肯定能达到自己人生的巅峰，尽享山登绝顶我为峰的豪迈与忘我。


另外需要考虑的情况是，如果代价函数是凸函数，那么就存在全局最优解，方圆五百里就只有一个山峰，那命中注定了，它就是你要找的唯一了。但如果是非凸的，那么就会有很多局部最优的解，有一望无际的山峰，人的视野是伟大的也是渺小的，你不知道哪个山峰才是最高的，可能你会被命运作弄，很无辜的陷入一个局部最优里面，坐井观天，以为自己找到的就是最好的。没想到山外有山，人外有人，光芒总在未知的远处默默绽放。但也许命运眷恋善良的你，带给你的总是最好的归宿。也有很多不信命的人，觉得人定胜天的人，誓要找到最好的，否则不会罢休，永不向命运妥协，除非自己有一天累了，倒下了，也要靠剩下的一口气，迈出一口气能支撑的路程。好悲凉啊……哈哈。

所以，正如上面所述，逻辑回归就是这样的一个过程：面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏，冥冥人海，滚滚红尘，我们是否找到了最适合的那个她。
        
### 逻辑回归（Logistic Regression）

假设我们的样本是{x, y}，y是0或者1，表示正类或者负类，x是我们的m维的样本特征向量。那么这个样本x属于正类，也就是y=1的“概率”可以通过下面的逻辑函数来表示：
	
![逻辑函数](http://img.blog.csdn.net/20140302234136062?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

这里θ是模型参数，也就是回归系数，σ是sigmoid函数。实际上这个函数是由下面的对数几率（也就是x属于正类的可能性和负类的可能性的比值的对数）变换得到的：

![手动推导一下吧](http://img.blog.csdn.net/20140302234157953?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


![](https://pic3.zhimg.com/v2-569e40432a36bb5c6524ec03d501892a_b.png)

![](https://pic1.zhimg.com/v2-56666b6e14927636bd71b71ba81d2e64_b.png)

*选用Sigmoid函数为Logistic回归函数的原因是：*

逻辑回归中使用到的sigmoid函数到底起到了什么作用。下图的例子中，需要判断肿瘤是恶性还是良性，其中横轴是肿瘤大小，纵轴是线性函数hw(x)=wTx+bhw(x)=wTx+b的取值，因此在左图中可以根据训练集(图中的红叉)找到一条决策边界，并且以0.5作为阈值，将hw(x)⩾0.5hw(x)⩾0.5情况预测为恶性肿瘤，这种方式在这种数据比较集中的情况下好用，但是一旦出现如右图中的离群点，它会导致学习到的线性函数偏离(它产生的权重改变量会比较大)，从而原先设定的0.5阈值就不好用了，此时要么调整阈值要么调整线性函数。如果我们调节阈值，在这个图里线性函数取值看起来是0～1，但是在其他情况下可能就是从−∞−∞到∞∞，所以阈值的大小很难确定，假如能够把wTx+bwTx+b的值变换到一个能控制的范围那么阈值就好确定了，所以找到了sigmoid函数，将wTx+bwTx+b值映射到了(0,1)，并且解释成概率。而如果调节线性函数，那么最需要的是减少离群点的影响，离群点往往会导致比较大的|wTx+b||wTx+b|值，通过sigmoid函数刚好能够削弱这种类型值的影响，这种值经过sigmoid之后接近0或者1，从而对wjwj的偏导数为hw(x(i))(1−hw(x(i)))x(i)jhw(x(i))(1−hw(x(i)))xj(i)，无论接近0还是1这个导数都是非常小的。因此可以说sigmoid在逻辑回归中起到了两个作用，一是将线性函数的结果映射到了(0,1)，一是减少了离群点的影响。

![](http://7xkmdr.com1.z0.glb.clouddn.com/lr5.jpg)

Sigmoid 函数在有个很漂亮的“S"形，如下图所示（引自维基百科）：
![](https://pic2.zhimg.com/v2-b6c0a14d298c4857cabc80bb27aecba1_b.png)


综合上述两式，我们得到逻辑回归模型的数学表达式：

![](https://pic2.zhimg.com/v2-8c8064514b0d41ebd9f3222d4fec169d_b.png)

Cost函数和J函数如下，它们是基于最大似然估计推导得到的。

![](https://pic2.zhimg.com/v2-8c8064514b0d41ebd9f3222d4fec169d_b.png)

下面详细说明推导的过程：

![](https://pic4.zhimg.com/v2-2a3c65b90fd23715566dee0420567baf_b.png)

最大似然估计就是求使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为下式，即：

![](https://pic1.zhimg.com/v2-36071a33031756d36876df600daaa644_b.png)


梯度下降法求的最小值:

![](https://pic3.zhimg.com/v2-631267a13f028b13e3c75b07a8b963f2_b.png)
